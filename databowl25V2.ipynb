{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Snap Game Prediction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.datasets import make_classification \n",
    "import xgboost as xgb\n",
    "\n",
    "# Function to drop unwanted features from the dataset\n",
    "def drop_unwanted_features(df, features_to_drop):\n",
    "    \"\"\"\n",
    "    Drops specified features (columns) from the DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame.\n",
    "    features_to_drop (list): List of column names to drop.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: A new DataFrame with the specified columns dropped.\n",
    "    \"\"\"\n",
    "    # Check if all columns to drop exist in the DataFrame\n",
    "    features_to_drop = [feature for feature in features_to_drop if feature in df.columns]\n",
    "    \n",
    "    # Drop the unwanted features\n",
    "    df_cleaned = df.drop(columns=features_to_drop)\n",
    "    \n",
    "    return df_cleaned\n",
    "    # Returns a DataFrame without the unwanted columns\n",
    "\n",
    "# Function to convert game clock time into seconds\n",
    "def convert_to_seconds(game_clock):\n",
    "    \"\"\"\n",
    "    Converts the game clock time (in 'MM:SS' format) to seconds.\n",
    "    \n",
    "    Parameters:\n",
    "    game_clock (str): The game clock time as a string in 'MM:SS' format.\n",
    "    \n",
    "    Returns:\n",
    "    int: The game clock time converted into seconds.\n",
    "    \"\"\"\n",
    "    # Split the clock string into minutes and seconds and convert to integers\n",
    "    minutes, seconds = map(int, game_clock.split(':'))\n",
    "    \n",
    "    # Convert the time into total seconds\n",
    "    return minutes * 60 + seconds\n",
    "\n",
    "# Function to calculate remaining time till the end of the half\n",
    "def calculate_remaining_time_end_of_half(row):\n",
    "    \"\"\"\n",
    "    Calculates the remaining time in seconds until the end of the half based on the game clock and half number.\n",
    "    \n",
    "    Parameters:\n",
    "    row (pd.Series): A row of the DataFrame containing 'gameClock' and 'half' columns.\n",
    "    \n",
    "    Returns:\n",
    "    int: The remaining time in seconds until the end of the half.\n",
    "    \"\"\"\n",
    "    half_game_time = 30 * 60  # 1800 seconds in a half (30 minutes * 60 seconds)\n",
    "    \n",
    "    # Convert the game clock to seconds for the current row\n",
    "    quarter_time_remaining = convert_to_seconds(row['gameClock'])\n",
    "    \n",
    "    # Calculate the total elapsed time in the game up to the current quarter\n",
    "    # Half 1 includes the first two quarters (each 15 minutes or 900 seconds), so we account for that\n",
    "    elapsed_time = (row['half'] - 1) * 900 + (900 - quarter_time_remaining)\n",
    "    \n",
    "    # Remaining time until the end of the half\n",
    "    return half_game_time - elapsed_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Data Understanding and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plays_df = pd.read_csv(r'C:\\Users\\Karahan C\\Desktop\\Portfolio Projects\\Kaggle\\nfl-big-data-bowl-2025\\plays.csv')\n",
    "games_df = pd.read_csv(r'C:\\Users\\Karahan C\\Desktop\\Portfolio Projects\\Kaggle\\nfl-big-data-bowl-2025\\games.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plays_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see this dataframe contains various data about plays as descriptive, post-snap or pre-snap. Since we investigate a pre-snap incident we could drop irrelevant features like playDescription.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plays_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plays_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = plays_df.merge(games_df[['gameId','homeTeamAbbr', 'visitorTeamAbbr']], on='gameId', how='left')\n",
    "# Merged two data set on gameIds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've merged the two data-set for is possesion team whether leading or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_drop = ['gameId','playId','isDropback', 'expectedPointsAdded', 'yardsGained', 'visitorTeamWinProbilityAdded',\n",
    "                    'homeTeamWinProbabilityAdded','prePenaltyYardsGained','penaltyYards','rushLocationType','qbSneak',\n",
    "                    'qbKneel','qbSpike','unblockedPressure','passTippedAtLine','timeToSack','timeInTackleBox','timeToThrow',\n",
    "                    'passLocationType','dropbackDistance','dropbackType','playAction','targetX', 'targetY','passLength',\n",
    "                    'playClockAtSnap', 'expectedPoints','preSnapVisitorTeamWinProbability','preSnapHomeTeamWinProbability',\n",
    "                    'playNullifiedByPenalty','yardlineSide','yardlineNumber','playDescription','pff_manZone','pff_passCoverage',\n",
    "                    'pff_runPassOption','pff_runConceptSecondary','pff_runConceptPrimary']\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In here I've droped unnecessary columns for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = drop_unwanted_features(df,features_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have tried to create my features in scope of situational football:\n",
    "- Is my team behind? I would tend to play because I would tie the fame asap.\n",
    "- Is my team needs to hurry for a TD because we are behind and run out of time?\n",
    "- Are we leading and inside last two mins? I would tend to run the ball because I gotta melt time.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Half feature created \n",
    "df_cleaned['half'] = df_cleaned['quarter'].apply(lambda row: 1 if row < 3 else 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remaing time feature created\n",
    "df_cleaned.loc[:, 'remainingTime'] = df_cleaned.apply(calculate_remaining_time_end_of_half, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cleaned dataset score data store as preSnapHomeScore and preSnapVisitor but team stored as possesionTeam and defensiveTeam.    \n",
    "\n",
    "df_cleaned['possessionTeamScore'] = df_cleaned.apply(lambda row: row['preSnapHomeScore'] if row['possessionTeam'] == row['homeTeamAbbr'] else row['preSnapVisitorScore'], axis=1)\n",
    "  \n",
    "df_cleaned['defensiveTeamScore'] = df_cleaned.apply(lambda row: row['preSnapVisitorScore'] if row['possessionTeam'] == row['homeTeamAbbr'] else row['preSnapHomeScore'], axis=1)\n",
    "\n",
    "# So I converted preSnapHomeScore and preSnapVisitor data to possessionTeamScore and defensiveTeamScore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This feature shows how many yards left for TD\n",
    "df_cleaned['yardsToTd'] = df_cleaned['absoluteYardlineNumber'] - 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This feature creates a score difference and shows that possesion team leading or not.  \n",
    "df_cleaned['possessionTeamLeadsBy'] = df_cleaned['possessionTeamScore'] - df_cleaned['defensiveTeamScore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned['isPossessionTeamLead'] = df_cleaned.apply(lambda row: 0 if row['possessionTeamLeadsBy'] > 0 else 1, axis=1 )\n",
    "df_cleaned['isPossessionTeamLead'] = df_cleaned['isPossessionTeamLead'].astype('byte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of column names\n",
    "new_order = ['half','quarter', 'gameClock','remainingTime', 'down', 'yardsToGo', 'yardsToTd','possessionTeamLeadsBy','isPossessionTeamLead',\n",
    "             'possessionTeam', 'defensiveTeam','homeTeamAbbr', 'visitorTeamAbbr','possessionTeamScore', 'defensiveTeamScore',\n",
    "             'preSnapHomeScore','preSnapVisitorScore', 'absoluteYardlineNumber', 'offenseFormation', 'receiverAlignment', 'passResult']\n",
    "\n",
    "# Reorder the columns in the DataFrame\n",
    "df_cleaned = df_cleaned[new_order]\n",
    "\n",
    "# Check the reordered DataFrame\n",
    "print(df_cleaned.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned['offenseFormation'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know some formations have tendencies for pass or run games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_cleaned.copy()  # Create a deep copy of the DataFrame\n",
    "heavyPass = ['SHOTGUN', 'EMPTY']\n",
    "df_cleaned['isHeavyRun/Pass'] = df_cleaned['offenseFormation'].apply(lambda formation: 1 if formation in heavyPass else 0)\n",
    "df_cleaned['isHeavyRun/Pass'] = df_cleaned['isHeavyRun/Pass'].astype('byte')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned['receiverAlignment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heavyRun = ['1x0', '2x0', '3x0', '1x1', '2x1']\n",
    "df_cleaned.loc[:, 'isWRsHeavyRun/Pass'] = df_cleaned['receiverAlignment'].apply(lambda alignment: 0 if alignment in heavyRun else 1)\n",
    "df_cleaned['isWRsHeavyRun/Pass'] = df_cleaned['isWRsHeavyRun/Pass'].astype('byte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This feature is bytes variable that show the game wheter game is pass (1) or not (0) \n",
    "df_cleaned['WasItPass'] = df_cleaned['passResult'].apply(lambda row: 0 if pd.isnull(row) == True else 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train - Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, n_informative=2, n_redundant=10, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train XGboost Model - Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_preds = []\n",
    "\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Store true and predicted labels \n",
    "    y_true.extend(y_test)\n",
    "    y_preds.extend(y_pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "y_pred_prob = y_pred\n",
    "auc = roc_auc_score(y_test, y_pred_prob)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "plt.plot(fpr, tpr, label=f'XGBoost (AUC = {auc:.2f})')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'gamma': [0, 0.1, 0.3],\n",
    "    'subsample': [0.7, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.8, 1.0]}\n",
    "\n",
    "xgb_clf = XGBClassifier(objective='binary:logistic')\n",
    "grid = GridSearchCV(xgb_clf, param_grid, cv=5, scoring='accuracy',verbose=1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\", grid.best_params_)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid.best_params_\n",
    "\n",
    "\n",
    "# Fit model with best parameters\n",
    "best_xgb_model = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "xgb.plot_importance(best_xgb_model)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_final = best_xgb_model.predict(X_test)\n",
    "accuracy_final = accuracy_score(y_test, y_pred_final)\n",
    "print(f'Final Model Accuracy: {accuracy_final}') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
